\subsection{Classes of Provenance Capture Method}
\begin{itemize}
\item Provenance API
\item Workflow Systems
\item Userland Tracing
\item OS Tracing
\end{itemize}

Provenance APIs and Workflow systems are not especially appealing given our requirements as both techniques require large amounts of user or programmer intervention.

\subsubsection{Userland Tracing}
\paragraph{FUSE}
Good filesystem level tracing, high overheads and requires that the user shadow their filesystem in a subdirectory.
\paragraph{libc interception}
Broad system call capture, low overheads, vulnerable to statically linked libraries and direct system calls.
\paragraph{ptrace}
Full system call interception, high overheads, arcane and abstruse interface.
\paragraph{Binary Rewriting}
Add trampolines to the binary being executed whenever a system call instruction is encountered. Provides full system call interposing capabilities. Difficult to implement correctly. Involves a lot of development time and effort.
\paragraph{Linux audit subsystem}
Powerful but rigidly specified capture capabilities, only a single configuration per machine allowed, system admin orientated. 
\paragraph{SystemTap}
Very powerful system call capture, provides scripting language with embedded C, can have high overheads, requires root permissions and requires Linux debug symbols to be installed.

\subsubsection{OS Tracing}
\begin{description}
\item[kprobes] Flexible kernel debugging and tracing, low overheads, requires root permissions, available only from 2.6.9 kernel and requires kernel to be compiled with CONFIG\_KPROBES enabled.
\item[System call table interception] Very powerful system call capture, requires root permissions, requires modifying page permissions, requires finding address of system call table.
\item[Linux security modules] Powerful capture capabilities, only a single KSM can be loaded (preventing users from also running SELinux), misses any system calls that fail before permissions checks.
\end{description}

\subsection{Database Paradigms}

\subsubsection{RDBMS}
RBDMS are conventionally used to store related records under a specific schema format. However they struggle to represent graph structures efficiently and result in exceedingly complex queries to traverse paths in such a database.

\subsubsection{Graph Databases}
Graph databases can represent our provenance object relationships effectively and allow for powerful querying capabilities. However most graph databases are distributed, proprietary or otherwise suited for much heavier weight systems. The initial phase of the project requires a simpler approach to represent provenance objects and hence graph databases have been ruled out for the first phase.

\subsubsection{Key/Value Stores}
Key/value stores are a quick and easy way to store provenance objects, they do not require a specific schema and provide a simple if less powerful interface for querying. Key/value stores are available for a range of applications from heavy weight distributed systems to single node systems using an embedded database. In phase one of the system we are focusing on a single node application and an embedded key/value store will cater to our requirements.

\subsection{Event Ordering}
Maintaining a correct view of the order of events in the system is important for a provenance system as small changes in the order of events can produce large changes in their effect.

Initially we were planning on timestamping all events as they occur, however the sytem clock on a given machine is not guarenteed to be sane over any given period due to NTP updates or the user manually changing the system time. Thus the system time is not sufficient for our needs as it does not give a consistent ordering.

Secondly we considered storing a counter in shared memory that all of the front-end systems would access to get sequence numbers for events that would produce an ordering over the events. This idea was discarded as multiple processes accessing the shared counter would require exclusive access using locks to prevent the possibility of duplicated values. Due to this the time between an event happening and it receiving a time stamp may vary wildly due to lock contention.

Finally we decided to use a hardware monotonic clock, this provides the guarentee of a consistant increasing time value over any single run of a given machine. The disadvantage of this clock is that it is reset when the machine is reboot but we think this can be solved by the fact that once our data is committed to the database its identifiers in the database will give it ordering in a global context.
